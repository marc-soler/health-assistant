import json
import logging
from llm_service import LLMService
from prompt_builder import PromptBuilder


class ResponseEvaluator:
    """
    ResponseEvaluator is responsible for evaluating the relevance of LLM-generated answers.
    It interacts with the LLMService to generate evaluations and processes the results.
    """

    def __init__(
        self,
        logger=None,
        llm_service=None,
        prompt_builder=None,
        evaluation_prompt_template=None,
    ):
        """
        Initializes the ResponseEvaluator with an instance of LLMService and a PromptBuilder.

        Args:
            llm_service (LLMService, optional): An instance of LLMService to interact with the LLM.
            prompt_builder (PromptBuilder, optional): An instance of PromptBuilder to build prompts.
            evaluation_prompt_template (str, optional): The template used to build the evaluation prompt.
        """
        self.llm_service = llm_service or LLMService()
        self.prompt_builder = prompt_builder or PromptBuilder(
            template=evaluation_prompt_template
        )
        self.logger = logger if logger is not None else logging.getLogger(__name__)

    def evaluate_relevance(self, question, answer):
        """
        Evaluates the relevance of an answer generated by the LLM for a given question.

        Args:
            question (str): The original question posed to the LLM.
            answer (str): The answer generated by the LLM.

        Returns:
            tuple: A tuple containing the evaluation result (dict) and token usage statistics.
        """
        # Use the PromptBuilder to construct the evaluation prompt
        try:
            prompt = self.prompt_builder.build_prompt(
                question=question, answer_llm=answer
            )
            self.logger.info("Evaluating relevance with generated prompt.")
        except ValueError as e:
            self.logger.error(f"Error building prompt: {e}")
            return {
                "Relevance": "UNKNOWN",
                "Explanation": f"Prompt construction failed: {e}",
            }

        try:
            evaluation, token_stats = self.llm_service.query_llm(
                prompt, model="gpt-4o-mini"
            )
            self.logger.info("Received evaluation response from LLM.")
        except Exception as e:
            self.logger.error(f"Failed to evaluate relevance: {e}")
            return {"Relevance": "UNKNOWN", "Explanation": "Evaluation failed"}, {}

        if evaluation is not None:
            try:
                json_eval = json.loads(evaluation)
                self.logger.info("Successfully parsed evaluation response.")
                return json_eval, token_stats
            except json.JSONDecodeError:
                self.logger.error("Failed to parse evaluation response as JSON.")
                result = {
                    "Relevance": "UNKNOWN",
                    "Explanation": "Failed to parse evaluation",
                }
                return result, token_stats
        else:
            self.logger.warning("No evaluation received from LLM.")
            result = {"Relevance": "UNKNOWN", "Explanation": "No evaluation received"}
            return result, token_stats
